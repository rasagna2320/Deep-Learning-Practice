{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51fbbf74",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-26T18:38:00.430893Z",
     "iopub.status.busy": "2026-01-26T18:38:00.430561Z",
     "iopub.status.idle": "2026-01-26T18:38:07.858775Z",
     "shell.execute_reply": "2026-01-26T18:38:07.857517Z"
    },
    "papermill": {
     "duration": 7.437997,
     "end_time": "2026-01-26T18:38:07.861272",
     "exception": false,
     "start_time": "2026-01-26T18:38:00.423275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\r\n",
      "Collecting pip\r\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\r\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pip\r\n",
      "  Attempting uninstall: pip\r\n",
      "    Found existing installation: pip 24.1.2\r\n",
      "    Uninstalling pip-24.1.2:\r\n",
      "      Successfully uninstalled pip-24.1.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "fastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed pip-25.3\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f3d483a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:38:07.873588Z",
     "iopub.status.busy": "2026-01-26T18:38:07.873234Z",
     "iopub.status.idle": "2026-01-26T18:38:10.548516Z",
     "shell.execute_reply": "2026-01-26T18:38:10.547452Z"
    },
    "papermill": {
     "duration": 2.683663,
     "end_time": "2026-01-26T18:38:10.550316",
     "exception": false,
     "start_time": "2026-01-26T18:38:07.866653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (3.10.0)\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras) (1.4.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras) (2.0.2)\r\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras) (14.2.0)\r\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras) (0.1.0)\r\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras) (3.15.1)\r\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras) (0.17.0)\r\n",
      "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras) (0.5.3)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras) (26.0rc2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras) (4.15.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (4.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (2.19.2)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55236723",
   "metadata": {
    "papermill": {
     "duration": 0.004453,
     "end_time": "2026-01-26T18:38:10.559457",
     "exception": false,
     "start_time": "2026-01-26T18:38:10.555004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### This code builds and trains a simple single-layer neural network in PyTorch to learn a linear mapping from 3 input features to 1 output using Mean Squared Error loss and the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4c7f480",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:38:10.571686Z",
     "iopub.status.busy": "2026-01-26T18:38:10.570738Z",
     "iopub.status.idle": "2026-01-26T18:38:22.298797Z",
     "shell.execute_reply": "2026-01-26T18:38:22.297591Z"
    },
    "papermill": {
     "duration": 11.737379,
     "end_time": "2026-01-26T18:38:22.301312",
     "exception": false,
     "start_time": "2026-01-26T18:38:10.563933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 0.7622030377388\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "x = torch.randn(100, 3)\n",
    "y = torch.randn(100, 1)\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = SimpleNet()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Final loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e1931e",
   "metadata": {
    "papermill": {
     "duration": 0.004687,
     "end_time": "2026-01-26T18:38:22.311004",
     "exception": false,
     "start_time": "2026-01-26T18:38:22.306317",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Simple Neural Network with TensorFlow\n",
    "\n",
    "This code creates and trains a minimal neural network using TensorFlow with a single dense layer to map 3 input features to 1 output using the Adam optimizer and Mean Squared Error loss.\n",
    "\n",
    "**Observation:**  \n",
    "Since the input and output data are randomly generated, the model does not learn any meaningful pattern—loss reduction only reflects fitting to noise, not real-world learning.\n",
    "`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6c41ec3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:38:22.322100Z",
     "iopub.status.busy": "2026-01-26T18:38:22.321620Z",
     "iopub.status.idle": "2026-01-26T18:38:49.715314Z",
     "shell.execute_reply": "2026-01-26T18:38:49.714311Z"
    },
    "papermill": {
     "duration": 27.401607,
     "end_time": "2026-01-26T18:38:49.717299",
     "exception": false,
     "start_time": "2026-01-26T18:38:22.315692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-26 18:38:24.657744: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769452704.903566      17 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769452704.983499      17 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769452705.602010      17 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769452705.602128      17 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769452705.602134      17 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769452705.602137      17 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-26 18:38:43.844595: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.5757\n",
      "Final loss: 1.5670843124389648\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Dummy data\n",
    "x = tf.random.normal((100, 3))\n",
    "y = tf.random.normal((100, 1))\n",
    "\n",
    "# Simple neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, input_shape=(3,))  # Single dense layer\n",
    "])\n",
    "\n",
    "# Compile model with optimizer and loss\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(x, y, epochs=100, verbose=0)\n",
    "\n",
    "print(\"Final loss:\", model.evaluate(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68be86f",
   "metadata": {
    "papermill": {
     "duration": 0.005482,
     "end_time": "2026-01-26T18:38:49.728177",
     "exception": false,
     "start_time": "2026-01-26T18:38:49.722695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Simple Neural Network Using Keras (tf.keras)\n",
    "\n",
    "This code builds and trains a basic Keras Sequential model with a single dense layer to learn a mapping from 3 input features to 1 output using the Adam optimizer and Mean Squared Error loss.\n",
    "\n",
    "**Observation:**  \n",
    "Because both `x` and `y` are randomly generated, the network is only fitting noise; any loss reduction does not indicate meaningful learning or generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7da72120",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:38:49.740343Z",
     "iopub.status.busy": "2026-01-26T18:38:49.739724Z",
     "iopub.status.idle": "2026-01-26T18:38:55.154562Z",
     "shell.execute_reply": "2026-01-26T18:38:55.153349Z"
    },
    "papermill": {
     "duration": 5.423217,
     "end_time": "2026-01-26T18:38:55.156542",
     "exception": false,
     "start_time": "2026-01-26T18:38:49.733325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.5423 \n",
      "Final loss: 1.6171876192092896\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "# Generate dummy data\n",
    "x = tf.random.normal((100, 3))\n",
    "y = tf.random.normal((100, 1))\n",
    "\n",
    "# Simple Keras model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(1, input_shape=(3,))  # Single dense layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(x, y, epochs=100, verbose=0)\n",
    "\n",
    "print(\"Final loss:\", model.evaluate(x, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db97d58f",
   "metadata": {
    "papermill": {
     "duration": 0.005537,
     "end_time": "2026-01-26T18:38:55.168203",
     "exception": false,
     "start_time": "2026-01-26T18:38:55.162666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Simple Perceptron (Single Neuron Implementation)\n",
    "\n",
    "This code implements a basic artificial neuron using NumPy, where inputs are combined with weights and a bias, then passed through a sigmoid activation function to produce an output between 0 and 1.\n",
    "\n",
    "**Observation:**  \n",
    "The neuron does not learn here—weights and bias are fixed—so the output purely reflects the predefined parameters and input values, making this a forward-pass–only perceptron.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "800839da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:38:55.181413Z",
     "iopub.status.busy": "2026-01-26T18:38:55.181084Z",
     "iopub.status.idle": "2026-01-26T18:38:55.188384Z",
     "shell.execute_reply": "2026-01-26T18:38:55.187233Z"
    },
    "papermill": {
     "duration": 0.015407,
     "end_time": "2026-01-26T18:38:55.189974",
     "exception": false,
     "start_time": "2026-01-26T18:38:55.174567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9990889488055994\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return sigmoid(total)\n",
    "\n",
    "# Initialize neuron parameters\n",
    "weights = np.array([0, 1])\n",
    "bias = 4\n",
    "\n",
    "# Create neuron and test\n",
    "n = Neuron(weights, bias)\n",
    "x = np.array([2, 3])\n",
    "\n",
    "print(n.feedforward(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b5ab7",
   "metadata": {
    "papermill": {
     "duration": 0.005198,
     "end_time": "2026-01-26T18:38:55.200817",
     "exception": false,
     "start_time": "2026-01-26T18:38:55.195619",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Perceptron Implementation for AND Gate\n",
    "\n",
    "This code implements a simple perceptron using a step activation function to simulate an AND logic gate, producing a binary output based on weighted inputs and a bias threshold.\n",
    "\n",
    "**Observation:**  \n",
    "The perceptron correctly models the AND gate because the problem is linearly separable; fixed weights and bias are sufficient without any training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7eece2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:38:55.212841Z",
     "iopub.status.busy": "2026-01-26T18:38:55.212502Z",
     "iopub.status.idle": "2026-01-26T18:38:55.220221Z",
     "shell.execute_reply": "2026-01-26T18:38:55.219219Z"
    },
    "papermill": {
     "duration": 0.016118,
     "end_time": "2026-01-26T18:38:55.222044",
     "exception": false,
     "start_time": "2026-01-26T18:38:55.205926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND Gate\n",
      "(0, 0) -> 0\n",
      "(0, 1) -> 0\n",
      "(1, 0) -> 0\n",
      "(1, 1) -> 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step activation function\n",
    "def step(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return step(total)\n",
    "\n",
    "# AND gate configuration\n",
    "weights = np.array([1, 1])\n",
    "bias = -1.5\n",
    "\n",
    "and_gate = Perceptron(weights, bias)\n",
    "\n",
    "print(\"AND Gate\")\n",
    "for x in [(0, 0), (0, 1), (1, 0), (1, 1)]:\n",
    "    print(x, \"->\", and_gate.predict(np.array(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f650fe98",
   "metadata": {
    "papermill": {
     "duration": 0.00544,
     "end_time": "2026-01-26T18:38:55.233275",
     "exception": false,
     "start_time": "2026-01-26T18:38:55.227835",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Perceptron Implementation for OR Gate\n",
    "\n",
    "This code configures a perceptron with appropriate weights and bias to model an OR logic gate, outputting 1 when at least one input is active.\n",
    "\n",
    "**Observation:**  \n",
    "Like the AND gate, the OR gate is linearly separable, allowing a single-layer perceptron with fixed parameters to represent it perfectly without learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fc03ac3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:38:55.245434Z",
     "iopub.status.busy": "2026-01-26T18:38:55.245110Z",
     "iopub.status.idle": "2026-01-26T18:38:55.251237Z",
     "shell.execute_reply": "2026-01-26T18:38:55.250231Z"
    },
    "papermill": {
     "duration": 0.014893,
     "end_time": "2026-01-26T18:38:55.253534",
     "exception": false,
     "start_time": "2026-01-26T18:38:55.238641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OR Gate\n",
      "(0, 0) -> 0\n",
      "(0, 1) -> 1\n",
      "(1, 0) -> 1\n",
      "(1, 1) -> 1\n"
     ]
    }
   ],
   "source": [
    "# OR gate parameters\n",
    "weights = np.array([1, 1])               \n",
    "bias = -0.5                            \n",
    "\n",
    "or_gate = Perceptron(weights, bias)\n",
    "\n",
    "# Testing OR gate\n",
    "print(\"\\nOR Gate\")\n",
    "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
    "    print(x, \"->\", or_gate.predict(np.array(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c58db10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:38:55.267304Z",
     "iopub.status.busy": "2026-01-26T18:38:55.266940Z",
     "iopub.status.idle": "2026-01-26T18:38:55.273719Z",
     "shell.execute_reply": "2026-01-26T18:38:55.272646Z"
    },
    "papermill": {
     "duration": 0.016,
     "end_time": "2026-01-26T18:38:55.275655",
     "exception": false,
     "start_time": "2026-01-26T18:38:55.259655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NAND Gate\n",
      "(0, 0) -> 1\n",
      "(0, 1) -> 1\n",
      "(1, 0) -> 1\n",
      "(1, 1) -> 0\n"
     ]
    }
   ],
   "source": [
    "# NAND gate configuration\n",
    "weights = np.array([-1, -1])\n",
    "bias = 1.5\n",
    "\n",
    "nand_gate = Perceptron(weights, bias)\n",
    "\n",
    "print(\"\\nNAND Gate\")\n",
    "for x in [(0, 0), (0, 1), (1, 0), (1, 1)]:\n",
    "    print(x, \"->\", nand_gate.predict(np.array(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93d50699",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:38:55.288452Z",
     "iopub.status.busy": "2026-01-26T18:38:55.288008Z",
     "iopub.status.idle": "2026-01-26T18:38:55.294201Z",
     "shell.execute_reply": "2026-01-26T18:38:55.293145Z"
    },
    "papermill": {
     "duration": 0.014822,
     "end_time": "2026-01-26T18:38:55.295903",
     "exception": false,
     "start_time": "2026-01-26T18:38:55.281081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOT Gate\n",
      "(0,) -> 1\n",
      "(1,) -> 0\n"
     ]
    }
   ],
   "source": [
    "# NOT gate configuration (single input)\n",
    "weights = np.array([-1])\n",
    "bias = 0.5\n",
    "\n",
    "not_gate = Perceptron(weights, bias)\n",
    "\n",
    "print(\"\\nNOT Gate\")\n",
    "for x in [(0,), (1,)]:\n",
    "    print(x, \"->\", not_gate.predict(np.array(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e86abd5",
   "metadata": {
    "papermill": {
     "duration": 0.005688,
     "end_time": "2026-01-26T18:38:55.307989",
     "exception": false,
     "start_time": "2026-01-26T18:38:55.302301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### NOR Gate (Perceptron)\n",
    "Implements logical NOR where the output is 1 only when both inputs are 0.  \n",
    "**Observation:** Like NAND, NOR is functionally complete and can be used to construct any logical operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8791c0fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:38:55.320617Z",
     "iopub.status.busy": "2026-01-26T18:38:55.320272Z",
     "iopub.status.idle": "2026-01-26T18:38:55.326702Z",
     "shell.execute_reply": "2026-01-26T18:38:55.325739Z"
    },
    "papermill": {
     "duration": 0.015061,
     "end_time": "2026-01-26T18:38:55.328678",
     "exception": false,
     "start_time": "2026-01-26T18:38:55.313617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOR Gate\n",
      "(0, 0) -> 1\n",
      "(0, 1) -> 0\n",
      "(1, 0) -> 0\n",
      "(1, 1) -> 0\n"
     ]
    }
   ],
   "source": [
    "# NOR gate configuration\n",
    "weights = np.array([-1, -1])\n",
    "bias = 0.5\n",
    "\n",
    "nor_gate = Perceptron(weights, bias)\n",
    "\n",
    "print(\"\\nNOR Gate\")\n",
    "for x in [(0, 0), (0, 1), (1, 0), (1, 1)]:\n",
    "    print(x, \"->\", nor_gate.predict(np.array(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3bc143",
   "metadata": {
    "papermill": {
     "duration": 0.005608,
     "end_time": "2026-01-26T18:38:55.340917",
     "exception": false,
     "start_time": "2026-01-26T18:38:55.335309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### XOR Gate Using a Two-Layer Neural Network\n",
    "\n",
    "This code implements an XOR logic gate using a manually designed two-layer neural network with sigmoid activation, where a hidden layer enables learning of non-linear decision boundaries.\n",
    "\n",
    "**Observation:**  \n",
    "XOR is not linearly separable, so a single perceptron fails; introducing a hidden layer allows the network to correctly model the XOR relationship even without training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05f7778d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:38:55.354421Z",
     "iopub.status.busy": "2026-01-26T18:38:55.354017Z",
     "iopub.status.idle": "2026-01-26T18:38:55.362859Z",
     "shell.execute_reply": "2026-01-26T18:38:55.362055Z"
    },
    "papermill": {
     "duration": 0.018138,
     "end_time": "2026-01-26T18:38:55.364924",
     "exception": false,
     "start_time": "2026-01-26T18:38:55.346786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Gate\n",
      "(0, 0) -> 0\n",
      "(0, 1) -> 0\n",
      "(1, 0) -> 0\n",
      "(1, 1) -> 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class XOR_Network:\n",
    "    def __init__(self):\n",
    "        # Hidden layer parameters\n",
    "        self.w1 = np.array([[1, 1], [1, 1]])\n",
    "        self.b1 = np.array([-0.5, -1.5])\n",
    "\n",
    "        # Output layer parameters\n",
    "        self.w2 = np.array([1, -2])\n",
    "        self.b2 = -0.5\n",
    "\n",
    "    def predict(self, x):\n",
    "        h = sigmoid(np.dot(self.w1, x) + self.b1)\n",
    "        output = sigmoid(np.dot(self.w2, h) + self.b2)\n",
    "        return 1 if output >= 0.5 else 0\n",
    "\n",
    "xor_gate = XOR_Network()\n",
    "\n",
    "print(\"XOR Gate\")\n",
    "for x in [(0, 0), (0, 1), (1, 0), (1, 1)]:\n",
    "    print(x, \"->\", xor_gate.predict(np.array(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678dc125",
   "metadata": {
    "papermill": {
     "duration": 0.005761,
     "end_time": "2026-01-26T18:38:55.376764",
     "exception": false,
     "start_time": "2026-01-26T18:38:55.371003",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### XOR Gate Using Multiple Perceptrons (Rule-Based Network)\n",
    "\n",
    "This code implements an XOR logic gate by combining two hidden perceptrons (OR and AND) and a final output perceptron, effectively composing linear models to achieve non-linear behavior.\n",
    "\n",
    "**Observation:**  \n",
    "Although XOR is not linearly separable, it can be solved by stacking perceptrons—showing how multi-layer networks overcome the limitations of single neurons without any training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a210451",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:38:55.389899Z",
     "iopub.status.busy": "2026-01-26T18:38:55.389596Z",
     "iopub.status.idle": "2026-01-26T18:38:55.397948Z",
     "shell.execute_reply": "2026-01-26T18:38:55.397007Z"
    },
    "papermill": {
     "duration": 0.01706,
     "end_time": "2026-01-26T18:38:55.399602",
     "exception": false,
     "start_time": "2026-01-26T18:38:55.382542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Gate\n",
      "(0, 0) -> 0\n",
      "(0, 1) -> 1\n",
      "(1, 0) -> 1\n",
      "(1, 1) -> 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step activation function\n",
    "def step(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "class XOR_Network:\n",
    "    def __init__(self):\n",
    "        # Hidden layer (OR and AND)\n",
    "        self.w_or = np.array([1, 1])\n",
    "        self.b_or = -0.5\n",
    "\n",
    "        self.w_and = np.array([1, 1])\n",
    "        self.b_and = -1.5\n",
    "\n",
    "        # Output layer\n",
    "        self.w_out = np.array([1, -2])\n",
    "        self.b_out = -0.5\n",
    "\n",
    "    def predict(self, x):\n",
    "        h1 = step(np.dot(self.w_or, x) + self.b_or)\n",
    "        h2 = step(np.dot(self.w_and, x) + self.b_and)\n",
    "        return step(self.w_out[0] * h1 + self.w_out[1] * h2 + self.b_out)\n",
    "\n",
    "xor_gate = XOR_Network()\n",
    "\n",
    "print(\"XOR Gate\")\n",
    "for x in [(0, 0), (0, 1), (1, 0), (1, 1)]:\n",
    "    print(x, \"->\", xor_gate.predict(np.array(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6580f302",
   "metadata": {
    "papermill": {
     "duration": 0.005449,
     "end_time": "2026-01-26T18:38:55.411118",
     "exception": false,
     "start_time": "2026-01-26T18:38:55.405669",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### XOR Training Using Gradient Descent (Neural Network from Scratch)\n",
    "\n",
    "This code trains a two-layer neural network using NumPy to learn the XOR function by applying forward propagation, backpropagation, and gradient descent without relying on any ML framework.\n",
    "\n",
    "**Observation:**  \n",
    "The gradual loss reduction shows that the network successfully learns a non-linear decision boundary, proving how hidden layers and gradient descent enable learning of problems that single perceptrons cannot solve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36141548",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:38:55.424438Z",
     "iopub.status.busy": "2026-01-26T18:38:55.424075Z",
     "iopub.status.idle": "2026-01-26T18:38:55.950979Z",
     "shell.execute_reply": "2026-01-26T18:38:55.949927Z"
    },
    "papermill": {
     "duration": 0.53629,
     "end_time": "2026-01-26T18:38:55.952913",
     "exception": false,
     "start_time": "2026-01-26T18:38:55.416623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3247\n",
      "Epoch 1000, Loss: 0.2406\n",
      "Epoch 2000, Loss: 0.1960\n",
      "Epoch 3000, Loss: 0.1207\n",
      "Epoch 4000, Loss: 0.0305\n",
      "Epoch 5000, Loss: 0.0125\n",
      "Epoch 6000, Loss: 0.0074\n",
      "Epoch 7000, Loss: 0.0051\n",
      "Epoch 8000, Loss: 0.0038\n",
      "Epoch 9000, Loss: 0.0031\n",
      "\n",
      "XOR Predictions after Training:\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# XOR dataset\n",
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Initialize parameters\n",
    "np.random.seed(42)\n",
    "W1 = np.random.rand(2, 2)\n",
    "b1 = np.random.rand(1, 2)\n",
    "W2 = np.random.rand(2, 1)\n",
    "b2 = np.random.rand(1, 1)\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    hidden_output = sigmoid(np.dot(X, W1) + b1)\n",
    "    y_pred = sigmoid(np.dot(hidden_output, W2) + b2)\n",
    "\n",
    "    error = y - y_pred\n",
    "    d_output = error * sigmoid_derivative(y_pred)\n",
    "    d_hidden = d_output.dot(W2.T) * sigmoid_derivative(hidden_output)\n",
    "\n",
    "    W2 += hidden_output.T.dot(d_output) * learning_rate\n",
    "    b2 += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
    "    W1 += X.T.dot(d_hidden) * learning_rate\n",
    "    b1 += np.sum(d_hidden, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(error ** 2)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Test trained model\n",
    "print(\"\\nXOR Predictions after Training:\")\n",
    "for i in range(len(X)):\n",
    "    print(X[i], \"->\", round(y_pred[i][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd34cb1b",
   "metadata": {
    "papermill": {
     "duration": 0.005865,
     "end_time": "2026-01-26T18:38:55.964620",
     "exception": false,
     "start_time": "2026-01-26T18:38:55.958755",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Boolean Functions Overview\n",
    "\n",
    "For two binary inputs `(x₁, x₂)`, there are **16 possible Boolean functions**.  \n",
    "Examples include AND, OR, NAND, NOR, XOR, XNOR, TRUE, FALSE, A, B, ¬A, ¬B, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7b36e8",
   "metadata": {
    "papermill": {
     "duration": 0.00561,
     "end_time": "2026-01-26T18:38:55.975901",
     "exception": false,
     "start_time": "2026-01-26T18:38:55.970291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## One-Layer Neural Network (Perceptron)\n",
    "\n",
    "A single-layer perceptron can represent **only linearly separable Boolean functions**.\n",
    "\n",
    "**Capabilities:**\n",
    "- AND, OR, NAND, NOR, A, B, ¬A, ¬B, TRUE, FALSE  \n",
    "- XOR, XNOR are not possible to derive results\n",
    "\n",
    "**Key Insight:**  \n",
    "One-layer networks fail when the decision boundary is non-linear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fcedbbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:38:55.989939Z",
     "iopub.status.busy": "2026-01-26T18:38:55.989040Z",
     "iopub.status.idle": "2026-01-26T18:38:55.997554Z",
     "shell.execute_reply": "2026-01-26T18:38:55.996603Z"
    },
    "papermill": {
     "duration": 0.017762,
     "end_time": "2026-01-26T18:38:55.999501",
     "exception": false,
     "start_time": "2026-01-26T18:38:55.981739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] -> 0\n",
      "[0 1] -> 0\n",
      "[1 0] -> 0\n",
      "[1 1] -> 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def step(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "class OneLayerNN:\n",
    "    def __init__(self):\n",
    "        self.w = np.random.randn(2)\n",
    "        self.b = np.random.randn()\n",
    "\n",
    "    def predict(self, x):\n",
    "        return step(np.dot(self.w, x) + self.b)\n",
    "\n",
    "#Here X remains same for different gate or boolean functin change the y \n",
    "X = np.array([\n",
    "    [0,0],\n",
    "    [0,1],\n",
    "    [1,0],\n",
    "    [1,1]\n",
    "])\n",
    "\n",
    "\n",
    "#AND Gate\n",
    "y_and = np.array([0, 0, 0, 1])\n",
    "\n",
    "model = OneLayerNN()\n",
    "model.w = np.array([1, 1])\n",
    "model.b = -1.5\n",
    "\n",
    "for x in X:\n",
    "    print(x, \"->\", model.predict(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a01bc23",
   "metadata": {
    "papermill": {
     "duration": 0.005965,
     "end_time": "2026-01-26T18:38:56.011815",
     "exception": false,
     "start_time": "2026-01-26T18:38:56.005850",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Two-Layer Neural Network (With Hidden Layer)\n",
    "\n",
    "A neural network with **one hidden layer** can represent **all 16 Boolean functions**.\n",
    "\n",
    "**Architecture:**\n",
    "- Inputs: 2  \n",
    "- Hidden layer: 2 neurons  \n",
    "- Output: 1 neuron  \n",
    "\n",
    "**Key Insight:**  \n",
    "The hidden layer enables learning of **non-linear decision boundaries**, making XOR and XNOR solvable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3a769a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:38:56.025102Z",
     "iopub.status.busy": "2026-01-26T18:38:56.024722Z",
     "iopub.status.idle": "2026-01-26T18:38:57.009778Z",
     "shell.execute_reply": "2026-01-26T18:38:57.008519Z"
    },
    "papermill": {
     "duration": 0.994132,
     "end_time": "2026-01-26T18:38:57.011731",
     "exception": false,
     "start_time": "2026-01-26T18:38:56.017599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Gate: \n",
      "Predictions:\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n",
      "\n",
      "XNOR Gate: \n",
      "Predictions:\n",
      "[0 0] -> 1\n",
      "[0 1] -> 0\n",
      "[1 0] -> 0\n",
      "[1 1] -> 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "class TwoLayerNN:\n",
    "    def __init__(self):\n",
    "        self.W1 = np.random.randn(2, 4)\n",
    "        self.b1 = np.zeros((1, 4))\n",
    "        self.W2 = np.random.randn(4, 1)\n",
    "        self.b2 = np.zeros((1, 1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.h = sigmoid(np.dot(X, self.W1) + self.b1)\n",
    "        self.y_pred = sigmoid(np.dot(self.h, self.W2) + self.b2)\n",
    "        return self.y_pred\n",
    "\n",
    "    def train(self, X, y, lr=0.1, epochs=10000):\n",
    "        for _ in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "\n",
    "            d_out = (y_pred - y) * sigmoid_derivative(y_pred)\n",
    "            d_hid = d_out.dot(self.W2.T) * sigmoid_derivative(self.h)\n",
    "\n",
    "            self.W2 -= self.h.T.dot(d_out) * lr\n",
    "            self.b2 -= np.sum(d_out, axis=0, keepdims=True) * lr\n",
    "            self.W1 -= X.T.dot(d_hid) * lr\n",
    "            self.b1 -= np.sum(d_hid, axis=0, keepdims=True) * lr\n",
    "\n",
    "\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "print(\"XOR Gate: \")\n",
    "# XOR Gate\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "model = TwoLayerNN()\n",
    "model.train(X, y)\n",
    "\n",
    "print(\"Predictions:\")\n",
    "preds = model.forward(X)\n",
    "for i in range(len(X)):\n",
    "    print(X[i], \"->\", round(preds[i][0]))\n",
    "\n",
    "\n",
    "print(\"\\nXNOR Gate: \")\n",
    "#NOR Gate\n",
    "y = np.array([[1],[0],[0],[1]])\n",
    "\n",
    "model = TwoLayerNN()\n",
    "model.train(X, y)\n",
    "\n",
    "print(\"Predictions:\")\n",
    "preds = model.forward(X)\n",
    "for i in range(len(X)):\n",
    "    print(X[i], \"->\", round(preds[i][0]))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 63.520963,
   "end_time": "2026-01-26T18:39:00.557277",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-26T18:37:57.036314",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
